{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89ccb57-cdc2-443f-92e6-ba1e939663eb",
   "metadata": {},
   "source": [
    "**Assignment 1 : WEB SCRAPING (AMAZON)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b729f4-ba43-4895-a3f2-4934249d99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Saved all reviews to reviewsagain.csv\n"
     ]
    }
   ],
   "source": [
    "# Nor Ardini Arwan - IS01082907\n",
    "# Alisa Nadia binti Ahmad -  SW01081353\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Defining the URL of the product page on Amazon\n",
    "url = 'https://www.amazon.com/Ceptics-World-Travel-Adapter-Kit/dp/B06XSJ14CX/ref=sr_1_1_sspa?dib=eyJ2IjoiMSJ9.I5Ir2F0qs8iND7IaupUWu3X1LpzEP0nbwGh6X0mijuyCEyk5DATgQrzOyW2jfeNF9Xr3kgaIuQ5mYVF88ebMz0-ORMxf4TLg1PRaZVrM0pzixYnpvNp2UwoCNoZ4tPvLVKh6Yk1yGVcfnDDeSmyro1HP3a9lELBEfLw4V3lrSaLM0Vlw7DsjOHlmD4mJ1JUh25q6Tmq5XRhIH_oh1fh_fP0btklYbtjswhTvoc6Z-wk.sfHtHnhVfv3DlAJMrPIBpOqtw6zx6HR28-H-9TOASXc&dib_tag=se&keywords=malaysia&qid=1711846761&sr=8-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&th=1'\n",
    "\n",
    "# Function to extract reviews from a page URL\n",
    "def extract_reviews(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    reviews = []\n",
    "    # Loop through each review on the page\n",
    "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
    "        try:\n",
    "            # Extract reviewer's name\n",
    "            r_reviewer_element = review.select_one(\"span.a-profile-name\")\n",
    "            r_reviewer = r_reviewer_element.text.strip() if r_reviewer_element else None\n",
    "        except AttributeError:\n",
    "            r_reviewer = None\n",
    "\n",
    "        try:\n",
    "            # Extracting review date\n",
    "            review_date_element = review.find('span', {'data-hook': 'review-date'})\n",
    "            review_date = review_date_element.text.strip()\n",
    "\n",
    "            date_pattern = re.compile(r\"(\\w+\\s\\d{1,2},\\s\\d{4})\")\n",
    "            match = re.search(date_pattern, review_date)\n",
    "            review_date = match.group() if match else \"Date not found\"\n",
    "\n",
    "        except AttributeError:\n",
    "            review_date = ''\n",
    "\n",
    "        try:\n",
    "            # Extracting review contents\n",
    "            review_content = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
    "        except AttributeError:\n",
    "            review_content = ''\n",
    "\n",
    "        # Appending the extracted information to the reviews list\n",
    "        reviews.append([r_reviewer, review_date, review_content])\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Function to save extracted reviews to a CSV file\n",
    "def save_reviews_to_csv(reviews, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Reviewer Name\", \"Review Date\", \"Review Content\"])\n",
    "        writer.writerows(reviews)\n",
    "\n",
    "# Main function for the scraping and saving process\n",
    "def main():\n",
    "    all_reviews = []\n",
    "    # Looping through the first 5 pages of reviews\n",
    "    for i in range(1, 6):\n",
    "        print(f'Scraping page {i}...')\n",
    "        # Construct the URL for the current page\n",
    "        page_url = url + f'&pageNumber={i}'\n",
    "        # Extract reviews from the current page and append them to the list of all reviews\n",
    "        reviews = extract_reviews(page_url)\n",
    "        all_reviews.extend(reviews)\n",
    "\n",
    "    # Save all reviews to a CSV file\n",
    "    save_reviews_to_csv(all_reviews, 'reviewlist.csv')\n",
    "    print('Saved all reviews to reviewlist.csv')\n",
    "\n",
    "# Execute the main function if the script is run directly\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1321d-71a2-4d6b-a6c2-a8b910adbe81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9ea0b-ae01-4f83-905a-a6100d4d0dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847434ba-0dd9-49e6-974f-32e18f8463f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be60576-ea01-4f6b-b0a1-93b601d8621c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
